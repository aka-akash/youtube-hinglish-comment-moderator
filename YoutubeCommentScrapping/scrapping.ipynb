{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id of the video whose comments need to be scrapped off\n",
    "videoId = \"Mk-yWIF_92s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from API_KEY import stored_api_key\n",
    "def commentScraper(vidID):\n",
    "\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    # api_service_name = \"youtube\"\n",
    "    # api_version = \"v3\"\n",
    "    API_KEY = stored_api_key\n",
    "\n",
    "    youtube = build(\n",
    "        \"youtube\", \"v3\", developerKey = API_KEY)\n",
    "\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"id, replies, snippet\",\n",
    "        # order=\"time\", #Order can be either time or relevance\n",
    "        maxResults=100, #By deafult, its value is 20\n",
    "        videoId=vidID\n",
    "    )\n",
    "    response = request.execute()\n",
    "    comments_list = response['items']\n",
    "\n",
    "    while response.get('nextPageToken', None):\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"id, replies, snippet\",\n",
    "            # order=\"time\", #Order can be either time or relevance\n",
    "            maxResults=100, #By deafult, its value is 20\n",
    "            videoId=vidID,\n",
    "            pageToken=response['nextPageToken'] #to get the comment on the next block\n",
    "        )\n",
    "        response = request.execute()\n",
    "        comments_list.extend(response['items'])\n",
    "      \n",
    "    \n",
    "\n",
    "    return comments_list\n",
    "\n",
    "#Enter your VideoID here\n",
    "total_comments = commentScraper(videoId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_comments_array = []\n",
    "for i in range(0, len(total_comments)):\n",
    "  current_scrapped_comment = []\n",
    "  current_scrapped_comment.append(total_comments[i]['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "  current_scrapped_comment.append(total_comments[i]['snippet']['topLevelComment']['snippet']['authorDisplayName'])\n",
    "  current_scrapped_comment.append(total_comments[i]['snippet']['topLevelComment']['snippet']['authorChannelId']['value'])\n",
    "  current_scrapped_comment.append(total_comments[i]['snippet']['topLevelComment']['snippet']['publishedAt'][:10])\n",
    "  current_scrapped_comment.append(total_comments[i]['snippet']['topLevelComment']['snippet']['likeCount'])\n",
    "  scraped_comments_array.append(current_scrapped_comment)\n",
    "\n",
    "  if total_comments[i].get('replies', None):\n",
    "    for j in range(0, len(total_comments[i]['replies']['comments'])):\n",
    "      current_scrapped_comment = []\n",
    "      current_scrapped_comment.append(total_comments[i]['replies']['comments'][j]['snippet']['textDisplay'])\n",
    "      current_scrapped_comment.append(total_comments[i]['replies']['comments'][j]['snippet']['authorDisplayName'])\n",
    "      current_scrapped_comment.append(total_comments[i]['replies']['comments'][j]['snippet']['authorChannelId']['value'])\n",
    "      current_scrapped_comment.append(total_comments[i]['replies']['comments'][j]['snippet']['publishedAt'][:10])\n",
    "      current_scrapped_comment.append(total_comments[i]['replies']['comments'][j]['snippet']['likeCount'])\n",
    "      scraped_comments_array.append(current_scrapped_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd  \n",
    "import pandas as pd  \n",
    "  \n",
    "     \n",
    "# list of comment, name, channelId, date, like\n",
    "comment = []\n",
    "name = []\n",
    "channelId = []\n",
    "date = []\n",
    "like = []\n",
    "\n",
    "for i in range(0, len(scraped_comments_array)):\n",
    "  comment.append(scraped_comments_array[i][0])\n",
    "  name.append(scraped_comments_array[i][1])\n",
    "  channelId.append(scraped_comments_array[i][2])\n",
    "  date.append(scraped_comments_array[i][3])\n",
    "  like.append(scraped_comments_array[i][4])\n",
    "     \n",
    "# dictionary of lists  \n",
    "dict = {'Comment':comment, 'AuthorName':name, 'AuthorChannelId':channelId, 'PublishedDate':date, 'LikeCount':like}  \n",
    "df = pd.DataFrame(dict) \n",
    "\n",
    "# saving the dataframe \n",
    "df.to_csv('YoutubeScrapped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
